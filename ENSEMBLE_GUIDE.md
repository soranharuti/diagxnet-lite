# Pair-wise Stacking Ensemble for DiagXNet-Lite

This module implements **pair-wise stacking ensemble learning** by combining predictions from two base models using a meta-learner.

## ğŸ¯ Overview

**Stacking** is an ensemble learning technique that:
1. Trains multiple base models independently
2. Uses base model predictions as features for a meta-learner
3. Meta-learner learns the optimal combination of predictions

## ğŸ—ï¸ Architecture

### Base Models (Choose One Pair)
1. **DenseNet-121** + **EfficientNet-B3**
2. **DenseNet-121** + **Inception-ResNet-V2**

### Meta-Learner Options
- **Neural Network**: 2-layer feedforward network (recommended)
- **Logistic Regression**: Simple linear combination

## ğŸ“Š Training Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 1: Train Base Models            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”‚
â”‚  Model 1 (DenseNet-121)                 â”‚
â”‚  Model 2 (EfficientNet-B3 / Inception)  â”‚
â”‚  â€¢ Trained independently on 60% data    â”‚
â”‚  â€¢ Validated on 20% data                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 2: Extract Predictions           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€               â”‚
â”‚  â€¢ Get predictions from both models     â”‚
â”‚  â€¢ On separate 20% meta-training set    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 3: Train Meta-Learner            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”‚
â”‚  â€¢ Input: Concatenated predictions      â”‚
â”‚  â€¢ Learn optimal combination             â”‚
â”‚  â€¢ Base models frozen (default)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### 1. Install Additional Dependencies

```bash
pip install timm  # For Inception-ResNet-V2
```

### 2. Train DenseNet-121 + EfficientNet-B3

```bash
python train_stacking_ensemble.py \
    --model1 densenet121 \
    --model2 efficientnet_b3 \
    --meta-learner neural_network \
    --epochs-base 5 \
    --epochs-meta 3 \
    --batch-size 16 \
    --lr 1e-4
```

### 3. Train DenseNet-121 + Inception-ResNet-V2

```bash
python train_stacking_ensemble.py \
    --model1 densenet121 \
    --model2 inception_resnet_v2 \
    --meta-learner neural_network \
    --epochs-base 5 \
    --epochs-meta 3
```

## ğŸ“ File Structure

```
diagxnet-lite/
â”œâ”€â”€ src/models/
â”‚   â”œâ”€â”€ architectures.py      # Base model architectures
â”‚   â”‚   â”œâ”€â”€ DenseNet-121
â”‚   â”‚   â”œâ”€â”€ EfficientNet-B3
â”‚   â”‚   â””â”€â”€ Inception-ResNet-V2
â”‚   â””â”€â”€ ensemble.py            # Ensemble models
â”‚       â”œâ”€â”€ StackingEnsemble   # Stacking with meta-learner
â”‚       â”œâ”€â”€ MetaLearner        # Neural network / logistic
â”‚       â””â”€â”€ WeightedAverage    # Simple weighted average (alternative)
â””â”€â”€ train_stacking_ensemble.py # Training script
```

## âš™ï¸ Configuration Options

### Command Line Arguments

| Argument | Default | Options | Description |
|----------|---------|---------|-------------|
| `--model1` | `densenet121` | Any architecture | First base model |
| `--model2` | `efficientnet_b3` | `efficientnet_b3`, `inception_resnet_v2` | Second base model |
| `--meta-learner` | `neural_network` | `neural_network`, `logistic` | Meta-learner type |
| `--epochs-base` | `5` | Any int | Epochs for base models |
| `--epochs-meta` | `3` | Any int | Epochs for meta-learner |
| `--batch-size` | `16` | Any int | Batch size |
| `--lr` | `1e-4` | Any float | Learning rate |

## ğŸ“ How It Works

### 1. Base Model Training

Each base model is trained independently:

```python
# Train Model 1
model1 = create_model("densenet121", num_classes=14)
train(model1, train_data)  # 60% of data

# Train Model 2  
model2 = create_model("efficientnet_b3", num_classes=14)
train(model2, train_data)  # 60% of data
```

### 2. Meta-Feature Generation

Extract predictions as features:

```python
# On meta-training set (20% of data)
pred1 = model1(X_meta)  # Shape: (N, 14)
pred2 = model2(X_meta)  # Shape: (N, 14)

# Concatenate
meta_features = torch.cat([pred1, pred2], dim=1)  # Shape: (N, 28)
```

### 3. Meta-Learner Training

Train meta-learner on base predictions:

```python
meta_learner = MetaLearner(
    num_base_models=2,
    num_classes=14,
    meta_learner_type="neural_network"
)

# Learn optimal combination
output = meta_learner([pred1, pred2])  # Shape: (N, 14)
```

## ğŸ“ˆ Expected Improvements

Ensemble learning typically provides:
- **+2-5% AUROC** improvement over single models
- **Better calibration** (more reliable confidence scores)
- **Reduced variance** (more stable predictions)
- **Complementary errors** (models make different mistakes)

## ğŸ” Evaluation

After training, evaluate the ensemble:

```python
from src.models.ensemble import create_ensemble

# Load trained models
ensemble = create_ensemble(model1, model2, ensemble_type="stacking")
ensemble.load_state_dict(torch.load("ensemble_best.pth"))

# Evaluate
ensemble.eval()
with torch.no_grad():
    output, base_outputs = ensemble(X_test)
    
# Compare with individual models
print(f"Model 1 AUROC: {auroc(y_test, base_outputs['model1'])}")
print(f"Model 2 AUROC: {auroc(y_test, base_outputs['model2'])}")
print(f"Ensemble AUROC: {auroc(y_test, output)}")
```

## ğŸ’¡ Tips

1. **Choose Diverse Models**: EfficientNet and DenseNet have different architectures
2. **Meta-Learner Type**: Neural network usually works better but takes longer
3. **Freeze Base Models**: Keep base models frozen during meta-training (default)
4. **Data Split**: 60/20/20 split ensures meta-learner sees unseen predictions

## ğŸ“š References

- **Stacking**: Wolpert, D. H. (1992). "Stacked generalization"
- **DenseNet**: Huang et al. (2017). "Densely Connected Convolutional Networks"
- **EfficientNet**: Tan & Le (2019). "EfficientNet: Rethinking Model Scaling"
- **Inception-ResNet**: Szegedy et al. (2017). "Inception-v4, Inception-ResNet"

## ğŸ¯ Next Steps

After training:
1. âœ… Evaluate ensemble vs individual models
2. âœ… Calculate AUROC, AUPRC, F1 for all 14 conditions
3. âœ… Analyze which conditions benefit most from ensemble
4. âœ… Generate Grad-CAM for ensemble predictions
5. âœ… Compare calibration curves

---

**Questions?** Check the main README or open an issue!
